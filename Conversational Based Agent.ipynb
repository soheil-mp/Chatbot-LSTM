{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKV2BEL76euo"
   },
   "source": [
    "<h1 style=\"text-align: center;text-transform: uppercase;\">Conversational Based Agent</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this project, you will build an end-to-end voice conversational agent, which can take a voice input audio line, and synthesize a response. The chatbot agent will be executed locally on your computer. \n",
    "\n",
    "<img style=\"width:550px; height:300px;\" src=\"assets/intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9fuw6EH4LOU"
   },
   "source": [
    "This jupyter notebook is consists of the following parts:\n",
    "1. __Speech Recognition:__ <br>In this part, you will create a speech recognition that can convert your voice into a text format.<br><br>\n",
    "2. __Chatbot:__ <br>This is the core of your conversational based agent. You will build a chatbot that will answer your questions. <br><br>\n",
    "3. __Text to Speech:__ <br>After getting the answer from your chatbot, it should be converted into a voice format and that is what you should create in this part. <br><br>\n",
    "4. __Finalize your Conversational Based Agent:__ <br>At the very end step, you will put everything together and create your Conversational Based Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx13lGaR6evT"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 1. Speech Recognition\n",
    "\n",
    "---\n",
    "\n",
    "In this part, we will use <a href=\"https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/\">Microsoft Azure</a> for performing the speech recognition. Using the Speech service is easy and affordable. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1. Create your Azure Account\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Before doing any speech recognition task, you need to follow these steps for setting up your account at Azure Microsoft:\n",
    "\n",
    "1. If you do not have a Microsoft account, you can sign up for one free of charge at the <a href=\"https://account.microsoft.com/account\">Microsoft account portal</a>. <br><br>\n",
    "\n",
    "2. Once you have a Microsoft account, go to the <a href=\"https://azure.microsoft.com/en-gb/free/ai/\">Azure sign-up page</a>, select Start free, and create a new Azure account using a Microsoft account. <br><br>\n",
    "\n",
    "3. Sign in to the <a href=\"https://portal.azure.com/\">Azure portal</a> using your Microsoft account. <br><br>\n",
    "\n",
    "4. Select Create a resource at the top left of the portal. If you do not see Create a resource, you can always find it by selecting the collapsed menu in the upper left:\n",
    "\n",
    "<img width=\"500px\" src=\"assets/collapsed-nav.png\">\n",
    "<br><br>\n",
    "\n",
    "5. In the New window, type \"speech\" in the search box and press ENTER. <br><br>\n",
    "\n",
    "6. In the search results, select Speech.\n",
    "\n",
    "<img width=\"700px\" src=\"assets/speech-search.png\">\n",
    "<br><br>\n",
    "\n",
    "7. Select Create, then:\n",
    "    - 7.1. Give a unique name for your new resource.\n",
    "    - 7.2. Choose the Azure subscription that the new resource is associated with to determine how the fees are billed.\n",
    "    - 7.3. Choose the <a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/regions\">region</a>.\n",
    "    - 7.4. Choose either a free (F0) or paid (S0) pricing tier.\n",
    "    - 7.5. Create a new resource group for this Speech subscription or assign the subscription to an existing resource group. Resource groups help you keep your various Azure subscriptions organized.\n",
    "    - 7.6. Select Create. This will take you to the deployment overview and display deployment progress messages.\n",
    "<br><br>\n",
    "\n",
    "It takes a few moments to deploy your new Speech resource. Once deployment is complete, select __Go to resource__ and in the left navigation pane select __Keys__ to display your Speech service subscription keys. Each subscription has two keys; you can use either key in your application. To quickly copy/paste a key to your code editor or other location, select the copy button next to each key, switch windows to paste the clipboard contents to the desired location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oK-eA2LN4LOX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2. Perform the Speech Recognition Task\n",
    "\n",
    "---\n",
    "\n",
    "This section shows you how to use the Speech Service through the Speech SDK for Python. It illustrates how the SDK can be used to recognize speech from microphone input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3EZOs5x4LOX"
   },
   "source": [
    "First, set up some general items. Import the Speech SDK Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V9MxWrjG4LOY"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMdwAu7O4LOc"
   },
   "source": [
    "Set up the subscription info for the Speech Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eQBeIFSU4LOd"
   },
   "outputs": [],
   "source": [
    "# Create an instance of a speech config with specified subscription key and service region.\n",
    "speech_key, service_region = \"YourSubscriptionKey\", \"YourServiceRegion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1mvYlbk4LOf"
   },
   "source": [
    "Create an instance of a speech config with specified subscription key and service region. Replace with your own subscription key and service region (e.g., \"westus\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KBgkSu4X4LOg"
   },
   "outputs": [],
   "source": [
    "# Create an instance of a speech config with specified subscription key and service region\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5eD0dCwS4LOj"
   },
   "source": [
    "Create a recognizer with the given settings. Since no explicit audio config is specified, the default microphone will be used (make sure the audio settings are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wdrqR4Ss4LOk"
   },
   "outputs": [],
   "source": [
    "# Creates a recognizer with the given settings\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config = speech_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQzqExEh4LOm"
   },
   "source": [
    "Starts speech recognition, and returns after a single utterance is recognized. The end of a single utterance is determined by listening for silence at the end or until a maximum of 15 seconds of audio is processed. The task returns the recognition text as result. \n",
    "\n",
    "__Note:__ Since `recognize_once()` returns only a single utterance, it is suitable only for single shot recognition like command or query. For long-running multi-utterance recognition, use `start_continuous_recognition()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr4eSAXj4LOn",
    "outputId": "c067ded0-c57f-4e1c-c4bd-0e965949eef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Speech recognition for a single utterance \n",
    "print(\"Say something...\")\n",
    "result = speech_recognizer.recognize_once()\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized speech: \t Hello, how are you doing?\n"
     ]
    }
   ],
   "source": [
    "print(\"Recognized speech: \\t\", result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIqinjUX6ewC"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 2. Chatbot\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this part, you will create a deep learning based conversational agent. This agent will be able to interact with users and understand their questions. More specifically, you will start with loading the dataset, cleaning and preprocessing them, and then you will feed them into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJi6wO7v4LOw"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1. Load and Clean the Chatterbot Dataset \n",
    "\n",
    "---\n",
    "\n",
    "In this project, we have provided you with multiple dataset files. Each of these files contains conversations regarding a specific topic. For example, topics about humor, food, movies, science, history, etc. You can read the description of each dataset in below:\n",
    "\n",
    "| Name of Dataset | Description |\n",
    "| :----:| :----: |\n",
    "| botprofile.yml | Personality of Your Chatbot |\n",
    "| humor.yml | Joke and Humor |\n",
    "| emotion.yml | Emotional Conversations |\n",
    "| politics.yml | Political Conversations |\n",
    "| ai.yml | General Questions about AI |\n",
    "| computers.yml | Conversations about Computer |\n",
    "| history.yml | Q&A about Historical Facts and Events |\n",
    "| psychology.yml | Psychological Conversations |\n",
    "| food.yml | Food Related Conversations. |\n",
    "| literature.yml | Conversations about Different Books, Authors, Genres |\n",
    "| money.yml | Conversations about Money, Investment, Economy |\n",
    "| trivia.yml | Conversations that Have Small Values |\n",
    "| gossip.yml | Gossipy Conversations |\n",
    "| conversations.yml | Common Conversations |\n",
    "| greetings.yml | Different Ways of Greeting |\n",
    "| sports.yml | Conversations about Sports. |\n",
    "| movies.yml | Conversation about Movies. |\n",
    "| science.yml | Conversations about Science  |\n",
    "| health.yml | Health Related Questions and Answers. |\n",
    "\n",
    "\n",
    "Feel free to modify these datasets in the way you want the chatbot to behave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKZLku6c4LOx"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import yaml\n",
    "from yaml import Loader\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYYGIupd4LO0"
   },
   "outputs": [],
   "source": [
    "# Function for loading all of the yml files\n",
    "def load_chatterbot_dataset():\n",
    "    \n",
    "    # Initialize empty lists for questions and answers\n",
    "    questions, answers = [], []\n",
    "    \n",
    "    # Get the list of all dataset names\n",
    "    dataset_names = glob.glob(\"datasets/chatterbot/*.yml\")\n",
    "    \n",
    "    # Iterate through each dataset name\n",
    "    for i_dataset_name in tqdm.tqdm(dataset_names):\n",
    "        \n",
    "        # Load the dataset\n",
    "        with open(i_dataset_name) as file:\n",
    "            greeting = yaml.load(file, Loader = Loader)[\"conversations\"]\n",
    "            \n",
    "        # Iterate through each conversation\n",
    "        for i_conversation in greeting:\n",
    "            \n",
    "            # If length is two\n",
    "            if len(i_conversation) == 2:\n",
    "                \n",
    "                # Append the question to 'questions' list\n",
    "                questions.append(i_conversation[0])\n",
    "                \n",
    "                # Append the answer to 'answers' list\n",
    "                answers.append(i_conversation[1])\n",
    "            \n",
    "            # If length is more than two\n",
    "            elif len(i_conversation) > 2:\n",
    "                \n",
    "                # Iterate through each index\n",
    "                for index in range (len(i_conversation)-1):\n",
    "                    \n",
    "                    # Append the question and answer\n",
    "                    questions.append(i_conversation[0])\n",
    "                    answers.append(i_conversation[index+1])\n",
    "                    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7693,
     "status": "ok",
     "timestamp": 1575580064620,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "iI3-jw9w4LO9",
    "outputId": "2ba271ef-f620-43f7-b56a-6c40af19fb72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 52.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the questions and answers\n",
    "questions_chatterbot, answers_chatterbot = load_chatterbot_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Question & Answers:  869\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Question & Answers: \", len(questions_chatterbot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.2. Load and Clean the Stanford Dataset \n",
    "\n",
    "---\n",
    "\n",
    "The second dataset that we are going to use, is called <a href=\"https://rajpurkar.github.io/SQuAD-explorer\">Stanford Question Answering Dataset (SQuAD 2.0)</a>. SQuAD is a reading comprehension dataset and a standard benchmark for QA models. THe dataset is publicly available on the website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('datasets/stanford question answering/dev-v2.0.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "# Get the training data\n",
    "train_data = [item for topic in train_data['data'] for item in topic['paragraphs'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stanford_dataset():\n",
    "    # Initialize the total questions and answers\n",
    "    total_questions, total_answers = [], []\n",
    "\n",
    "    # Iterate through train_data\n",
    "    for td in train_data:\n",
    "\n",
    "        # Get the list of questions and answers\n",
    "        qas = td[\"qas\"]\n",
    "\n",
    "        # Iterate through each question and answer\n",
    "        for i_qas in qas:\n",
    "\n",
    "            # Get the question\n",
    "            question = i_qas[\"question\"]\n",
    "            \n",
    "            # Get \"answers\" if it exists\n",
    "            if i_qas[\"answers\"]:\n",
    "                answer = i_qas[\"answers\"][0][\"text\"]\n",
    "                \n",
    "            # Get \"plausible_answers\" if it exists\n",
    "            elif i_qas[\"plausible_answers\"]:\n",
    "                answer = i_qas[\"plausible_answers\"][0][\"text\"]\n",
    "                \n",
    "            # If none of above exists then go to the beginning of the loop\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Append the questions and answers into the total questions and answers\n",
    "            total_questions.append(question)\n",
    "            total_answers.append(answer)\n",
    "            \n",
    "    return total_questions, total_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the questions and answers\n",
    "questions_stanford, answers_stanford = load_stanford_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Question & Answers:  11858\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Question & Answers: \", len(questions_stanford))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.3. Combine all Datasets\n",
    "\n",
    "---\n",
    "\n",
    "Now let's combine the Chatterbot dataset with Stanford Question Answering Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Question & Answers:  12727\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "questions = questions_chatterbot + questions_stanford\n",
    "answers = answers_chatterbot + answers_stanford\n",
    "print(\"Total Question & Answers: \", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Question & Answers:  8500\n"
     ]
    }
   ],
   "source": [
    "# Get a smaller sample of dataset (for memory purposes)\n",
    "sample_size = 8500\n",
    "questions = questions[:sample_size]\n",
    "answers = answers[:sample_size]\n",
    "print(\"Total Question & Answers: \", len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPDgm3I66exF"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4. Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "After cleaning the dataset, you should preprocess the dataset by following the below steps:\n",
    "\n",
    "1. Lower case the text.\n",
    "2. Decontract the text (e.g. she's -> she is, they're -> they are, etc.).\n",
    "3. Remove the punctuation (e.g. !, ?, $, %, #, @, ^, etc.).\n",
    "4. Tokenization.\n",
    "5. Pad the sequences to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6642,
     "status": "ok",
     "timestamp": 1575580089447,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "5qJaF_hI6exG",
    "outputId": "84a518e3-adc1-4da7-a7eb-ac73f4259008"
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L6ihwP5f6exI"
   },
   "outputs": [],
   "source": [
    "# Function for preprocessing the given text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Decontracting the text (e.g. it's -> it is)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:00<00:00, 75283.49it/s]\n",
      "100%|██████████| 869/869 [00:00<00:00, 65740.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the questions\n",
    "questions_preprocessed = []\n",
    "for i_question in tqdm(questions):\n",
    "    questions_preprocessed.append(preprocess_text(i_question))\n",
    "    \n",
    "# Preprocess the answers\n",
    "answers_preprocessed = []\n",
    "for i_answer in tqdm(answers):\n",
    "    answers_preprocessed.append(preprocess_text(i_answer))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1575580095488,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "hQ5J_WQ36exP",
    "outputId": "0aac5118-815d-4a81-f7e7-7876f5f84941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: \n",
      " have you read the communist\n",
      "\n",
      "Answer 0: \n",
      " yes  marx had made some interesting observations \n",
      "--------------------------------------------------------------------------\n",
      "Question 1: \n",
      " what is a government\n",
      "\n",
      "Answer 1: \n",
      " ideally it is a representative of the people \n",
      "--------------------------------------------------------------------------\n",
      "Question 2: \n",
      " what is greenpeace\n",
      "\n",
      "Answer 2: \n",
      " global organization promoting environmental activism \n",
      "--------------------------------------------------------------------------\n",
      "Question 3: \n",
      " what is capitalism\n",
      "\n",
      "Answer 3: \n",
      " the economic system in which all or most of the means of production and distribution  as land  factories  railroads  etc   are privately owned and operated for profit  originally under fully competitive conditions \n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the preprocessed questions and answers\n",
    "for i in range(4):\n",
    "    print(\"Question {}: \\n\".format(i), questions_preprocessed[i])\n",
    "    print(\"\")\n",
    "    print(\"Answer {}: \\n\".format(i), answers_preprocessed[i])\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that every training example are the type string, we need to first filter out both answers and questions that are not string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rgu9h3Ga4LPU"
   },
   "outputs": [],
   "source": [
    "answers_with_tags = list()\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPRw_ueq6ex-"
   },
   "source": [
    "After preprocessing the dataset, we should add a start tag (e.g. `<START>`) and an end tag (e.g. `<END>`) to answers. Remember that we will only add these tags to answers and not questions. This requirement is because of the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xan8xpTC6eyC"
   },
   "outputs": [],
   "source": [
    "# Add <START> and <END> tag to each sentence\n",
    "answers = list()\n",
    "for i in range(len(answers_with_tags)):\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1575580101816,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "T-gutAKp6eyD",
    "outputId": "3f620206-f516-4865-eb03-f9ba206b47bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> yes, marx had made some interesting observations. <END>',\n",
       " '<START> ideally it is a representative of the people. <END>',\n",
       " '<START> global organization promoting environmental activism. <END>',\n",
       " '<START> the economic system in which all or most of the means of production and distribution, as land, factories, railroads, etc., are privately owned and operated for profit, originally under fully competitive conditions. <END>',\n",
       " '<START> an established system of political administration by which a nation, state, district, etc. is governed. <END>']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxbjI3zBU-b1"
   },
   "source": [
    "Now it's time to tokenize our dataset. We use a class in Keras which allows us to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 793,
     "status": "ok",
     "timestamp": 1575580108328,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "p9FnHJsA6eyF",
    "outputId": "7d3d127b-309d-46bf-ea62-d4956b13b7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 1975\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit the tokenizer to questions and answers\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Get the total vocab size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1575580109511,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "sC6cafRA6eyI",
    "outputId": "bc954abd-b859-4eb9-cb4b-9639b3c0fca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 22) 22\n"
     ]
    }
   ],
   "source": [
    "### encoder input data\n",
    "\n",
    "# Tokenize the questions\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_questions = pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "\n",
    "print(encoder_input_data.shape, maxlen_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1575580110574,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "1urkx83k6eyJ",
    "outputId": "5edc1e24-8113-4327-935e-3bcb07982b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 45) 45\n"
     ]
    }
   ],
   "source": [
    "### decoder input data\n",
    "\n",
    "# Tokenize the answers\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "\n",
    "print(decoder_input_data.shape, maxlen_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1575580112277,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "hvq32nEI6eyL",
    "outputId": "14eb24de-e193-4151-d3d2-6a947e432621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 45, 1975)\n"
     ]
    }
   ],
   "source": [
    "### decoder_output_data\n",
    "\n",
    "# Iterate through index of tokenized answers\n",
    "for i in range(len(tokenized_answers)) :\n",
    "\n",
    "    #\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "\n",
    "# Pad the tokenized answers\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen = maxlen_answers, padding = 'post')\n",
    "\n",
    "# One hot encode\n",
    "onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
    "\n",
    "# Convert to numpy array\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7Cz8WVO4LP9"
   },
   "outputs": [],
   "source": [
    "# Saving all the arrays to storage\n",
    "np.save(\"enc_in_data.npy\", encoder_input_data)\n",
    "np.save(\"dec_in_data.npy\", decoder_input_data)\n",
    "np.save(\"dec_tar_data.npy\", decoder_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ClM9JxbyeJ0C"
   },
   "outputs": [],
   "source": [
    "# Load all the arrays from storage\n",
    "encoder_input_data = np.load(\"enc_in_data.npy\")\n",
    "decoder_input_data = np.load(\"dec_in_data.npy\")\n",
    "decoder_output_data = np.load(\"dec_tar_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCTc1O_meJ0F"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5. Train the Seq2Seq Model\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will use an architecture called Sequence to Sequence (or Seq2Seq). This model is used since the length of the input sequence (question) does not match the length of the output sequence (answer). This model is consists of an encoder and a decoder.\n",
    "- __Encoder:__ <br> In this part of the network, we take the input data and train on it. Then we pass the last state of the recurrent layer to decoder. <br><br>\n",
    "- __Decoder:__ <br> In this part of the network, we take the last state in encoder’s last recurrent layer. Then we will use it as an initial state in decoder's first recurrent layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's start by importing all the necessary libraries in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w521SVsleJ0F"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq9ZLFSEKmNi"
   },
   "source": [
    "Below you can play around with hyperparameters for improving the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hvhvarggeJ0P"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL28XUCmLb93"
   },
   "source": [
    "In the following block of code, you will implement the Encoder. You can follow the below steps for creating the encoder: \n",
    "\n",
    "1.   Create an input for the Encoder.\n",
    "2.   Create an embedding layer.\n",
    "3.   Create an LSTM layer which also returns the states.\n",
    "4.   Get the hidden state (state h) and cell state (state c) inside a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlV38YEgeJ0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khu834/anaconda/envs/chatbot/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/khu834/anaconda/envs/chatbot/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/khu834/anaconda/envs/chatbot/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "### Encoder Input\n",
    "embed_dim = 200\n",
    "num_lstm = 200\n",
    "\n",
    "# Input for encoder\n",
    "encoder_inputs = Input(shape = (None, ))\n",
    "\n",
    "# Embedding layer\n",
    "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True)(encoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states in addition to output)\n",
    "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True)(encoder_embedding)\n",
    "\n",
    "# Get the states for encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svSUWa7NOlKq"
   },
   "source": [
    "After creating your encoder, it's time to implement the decoder. You can follow the below steps for implementing the decoder:\n",
    "\n",
    "1.   Create an input for the decoder.\n",
    "2.   Create an embedding layer.\n",
    "3.   Create an LSTM layer that returns states and sequences.\n",
    "4.   Create a dense layer.\n",
    "5.   Get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-UaTlPLDeJ0R"
   },
   "outputs": [],
   "source": [
    "### Decoder\n",
    "\n",
    "# Input for decoder\n",
    "decoder_inputs = Input(shape = (None,  ))\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = 200 , mask_zero = True)(decoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states and sequences as well)\n",
    "decoder_lstm = LSTM(units = 200 , return_state = True , return_sequences = True)\n",
    "\n",
    "# Get the output of LSTM layer, using the initial states from the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax) \n",
    "\n",
    "# Get the output of Dense layer\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaOFGnnoQynS"
   },
   "source": [
    "Now that you have implemented the encoder and decoder. It's time to create your model which takes two inputs: encoder's input and decoder's input. Then it outputs the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8k0ErcMZeJ0T"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VamQBht8eJ0U"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = RMSprop(lr = LEARNING_RATE), loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1575581175908,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "CKrCrmA26eyQ",
    "outputId": "49f944e7-a3ae-4e33-9622-755d28fd8b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    395000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    395000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1975)   396975      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,828,575\n",
      "Trainable params: 1,828,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x = [encoder_input_data , decoder_input_data], \n",
    "          y = decoder_output_data, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          epochs = EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2870,
     "status": "ok",
     "timestamp": 1575581735556,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "9ERetSNY4LRH",
    "outputId": "d52d02a7-54ed-478e-e0f3-604e8f534884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model.save(filepath = './saved models/final_weight.h5') \n",
    "print(\"Model Weight Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1575581737849,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "tkVPvGd74LRK",
    "outputId": "5a3a9d6e-4684-417d-f7ae-8824b57f0a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "model.load_weights('./saved models/final_weight.h5') \n",
    "print(\"Model Weight Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F33sverZeJ0b"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.6. Inference\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to use our model for inference. In other words, we will ask a question to our chatbot and it will answer us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GaDAPDOa6eyb"
   },
   "outputs": [],
   "source": [
    "# Function for making inference\n",
    "def make_inference_models():\n",
    "    \n",
    "    # Create a model that takes encoder's input and outputs the states for encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
    "    decoder_state_input_h = Input(shape = (200, ))\n",
    "    decoder_state_input_c = Input(shape = (200, ))\n",
    "    \n",
    "    # Store the two inputs for decoder inside a list\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Pass the inputs through LSTM layer you have created before\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
    "    \n",
    "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Pass the output from LSTM layer through the dense layer you have created before\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NWYFiofO6eye"
   },
   "outputs": [],
   "source": [
    "# Function for converting strings to tokens\n",
    "def str_to_tokens(sentence:str):\n",
    "\n",
    "    # Lowercase the sentence and split it into words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Initialize a list for tokens\n",
    "    tokens_list = list()\n",
    "\n",
    "    # Iterate through words\n",
    "    for word in words:\n",
    "\n",
    "        # Append the word index inside tokens list\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "\n",
    "    # Pad the sequences to be the same length\n",
    "    return pad_sequences([tokens_list] , maxlen = maxlen_questions, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : Hello\n",
      " hi \n",
      "Enter question : Hi\n",
      " hello \n",
      "Enter question : How are you doing?\n",
      " i am doing well how about you \n",
      "Enter question : Can i ask you a question?\n",
      " sure ask \n",
      "Enter question : What are your interests?\n",
      " i am interested in all kinds of things we can talk about anything \n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for inference\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "# Iterate through the number of times you want to ask question\n",
    "for _ in range(5):\n",
    "\n",
    "    # Get the input and predict it with the encoder model\n",
    "    states_values = enc_model.predict(str_to_tokens(preprocess_text(input('Enter question : '))))\n",
    "\n",
    "    # Initialize the target sequence with zero - array([[0.]])\n",
    "    empty_target_seq = np.zeros(shape = (1, 1))\n",
    "\n",
    "    # Update the target sequence with index of \"start\"\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index[\"start\"]\n",
    "\n",
    "    # Initialize the stop condition with False\n",
    "    stop_condition = False\n",
    "\n",
    "    # Initialize the decoded words with an empty string\n",
    "    decoded_translation = ''\n",
    "\n",
    "    # While stop_condition is false\n",
    "    while not stop_condition :\n",
    "\n",
    "        # Predict the (target sequence + the output from encoder model) with decoder model\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "\n",
    "        # Get the index for sampled word\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "\n",
    "        # Initialize the sampled word with None\n",
    "        sampled_word = None\n",
    "\n",
    "        # Iterate through words and their indexes\n",
    "        for word, index in tokenizer.word_index.items() :\n",
    "\n",
    "            # If the index is equal to sampled word's index\n",
    "            if sampled_word_index == index :\n",
    "\n",
    "                # Add the word to the decoded string\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "\n",
    "                # Update the sampled word\n",
    "                sampled_word = word\n",
    "        \n",
    "        # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "\n",
    "            # Make the stop_condition to true\n",
    "            stop_condition = True\n",
    "            \n",
    "        # Initialize back the target sequence to zero - array([[0.]])    \n",
    "        empty_target_seq = np.zeros(shape = (1, 1))  \n",
    "\n",
    "        # Update the target sequence with index of \"start\"\n",
    "        empty_target_seq[0, 0] = sampled_word_index\n",
    "\n",
    "        # Get the state values\n",
    "        states_values = [h, c] \n",
    "\n",
    "    # Print the decoded string\n",
    "    print(decoded_translation[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-z2MXSd6ezL"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 4. Text to Speech\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will use a library called pyttsx3 which does the text-to-speech conversion. Unlike alternative libraries, this works offline and is compatible with both Python 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UOU6Y0DK6ezM"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wQ-pfu4t4LR7"
   },
   "outputs": [],
   "source": [
    "# Construct a new TTS engine instance\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVD7Nu4o4LSE",
    "outputId": "fa4f114c-ea72-45a8-a254-6d9e39bc9fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice 0: \n",
      " - ID: com.apple.speech.synthesis.voice.Alex\n",
      " - Name: Alex\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 1: \n",
      " - ID: com.apple.speech.synthesis.voice.alice\n",
      " - Name: Alice\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 2: \n",
      " - ID: com.apple.speech.synthesis.voice.alva\n",
      " - Name: Alva\n",
      " - Languages: ['sv_SE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 3: \n",
      " - ID: com.apple.speech.synthesis.voice.amelie\n",
      " - Name: Amelie\n",
      " - Languages: ['fr_CA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 4: \n",
      " - ID: com.apple.speech.synthesis.voice.anna\n",
      " - Name: Anna\n",
      " - Languages: ['de_DE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 5: \n",
      " - ID: com.apple.speech.synthesis.voice.carmit\n",
      " - Name: Carmit\n",
      " - Languages: ['he_IL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 6: \n",
      " - ID: com.apple.speech.synthesis.voice.damayanti\n",
      " - Name: Damayanti\n",
      " - Languages: ['id_ID']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 7: \n",
      " - ID: com.apple.speech.synthesis.voice.daniel.premium\n",
      " - Name: Daniel\n",
      " - Languages: ['en_GB']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 8: \n",
      " - ID: com.apple.speech.synthesis.voice.diego\n",
      " - Name: Diego\n",
      " - Languages: ['es_AR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 9: \n",
      " - ID: com.apple.speech.synthesis.voice.ellen\n",
      " - Name: Ellen\n",
      " - Languages: ['nl_BE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 10: \n",
      " - ID: com.apple.speech.synthesis.voice.fiona\n",
      " - Name: Fiona\n",
      " - Languages: ['en-scotland']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 11: \n",
      " - ID: com.apple.speech.synthesis.voice.Fred\n",
      " - Name: Fred\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 30\n",
      "\n",
      "Voice 12: \n",
      " - ID: com.apple.speech.synthesis.voice.ioana\n",
      " - Name: Ioana\n",
      " - Languages: ['ro_RO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 13: \n",
      " - ID: com.apple.speech.synthesis.voice.joana\n",
      " - Name: Joana\n",
      " - Languages: ['pt_PT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 14: \n",
      " - ID: com.apple.speech.synthesis.voice.jorge\n",
      " - Name: Jorge\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 15: \n",
      " - ID: com.apple.speech.synthesis.voice.juan\n",
      " - Name: Juan\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 16: \n",
      " - ID: com.apple.speech.synthesis.voice.kanya\n",
      " - Name: Kanya\n",
      " - Languages: ['th_TH']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 17: \n",
      " - ID: com.apple.speech.synthesis.voice.karen\n",
      " - Name: Karen\n",
      " - Languages: ['en_AU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 18: \n",
      " - ID: com.apple.speech.synthesis.voice.kyoko\n",
      " - Name: Kyoko\n",
      " - Languages: ['ja_JP']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 19: \n",
      " - ID: com.apple.speech.synthesis.voice.laura\n",
      " - Name: Laura\n",
      " - Languages: ['sk_SK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 20: \n",
      " - ID: com.apple.speech.synthesis.voice.lekha\n",
      " - Name: Lekha\n",
      " - Languages: ['hi_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 21: \n",
      " - ID: com.apple.speech.synthesis.voice.luca\n",
      " - Name: Luca\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 22: \n",
      " - ID: com.apple.speech.synthesis.voice.luciana\n",
      " - Name: Luciana\n",
      " - Languages: ['pt_BR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 23: \n",
      " - ID: com.apple.speech.synthesis.voice.maged\n",
      " - Name: Maged\n",
      " - Languages: ['ar_SA']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 24: \n",
      " - ID: com.apple.speech.synthesis.voice.mariska\n",
      " - Name: Mariska\n",
      " - Languages: ['hu_HU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 25: \n",
      " - ID: com.apple.speech.synthesis.voice.mei-jia\n",
      " - Name: Mei-Jia\n",
      " - Languages: ['zh_TW']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 26: \n",
      " - ID: com.apple.speech.synthesis.voice.melina\n",
      " - Name: Melina\n",
      " - Languages: ['el_GR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 27: \n",
      " - ID: com.apple.speech.synthesis.voice.milena\n",
      " - Name: Milena\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 28: \n",
      " - ID: com.apple.speech.synthesis.voice.moira\n",
      " - Name: Moira\n",
      " - Languages: ['en_IE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 29: \n",
      " - ID: com.apple.speech.synthesis.voice.monica\n",
      " - Name: Monica\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 30: \n",
      " - ID: com.apple.speech.synthesis.voice.nora\n",
      " - Name: Nora\n",
      " - Languages: ['nb_NO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 31: \n",
      " - ID: com.apple.speech.synthesis.voice.paulina\n",
      " - Name: Paulina\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 32: \n",
      " - ID: com.apple.speech.synthesis.voice.samantha\n",
      " - Name: Samantha\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 33: \n",
      " - ID: com.apple.speech.synthesis.voice.sara\n",
      " - Name: Sara\n",
      " - Languages: ['da_DK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 34: \n",
      " - ID: com.apple.speech.synthesis.voice.satu\n",
      " - Name: Satu\n",
      " - Languages: ['fi_FI']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 35: \n",
      " - ID: com.apple.speech.synthesis.voice.sin-ji\n",
      " - Name: Sin-ji\n",
      " - Languages: ['zh_HK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 36: \n",
      " - ID: com.apple.speech.synthesis.voice.tessa\n",
      " - Name: Tessa\n",
      " - Languages: ['en_ZA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 37: \n",
      " - ID: com.apple.speech.synthesis.voice.thomas\n",
      " - Name: Thomas\n",
      " - Languages: ['fr_FR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 38: \n",
      " - ID: com.apple.speech.synthesis.voice.ting-ting\n",
      " - Name: Ting-Ting\n",
      " - Languages: ['zh_CN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 39: \n",
      " - ID: com.apple.speech.synthesis.voice.veena\n",
      " - Name: Veena\n",
      " - Languages: ['en_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 40: \n",
      " - ID: com.apple.speech.synthesis.voice.Victoria\n",
      " - Name: Victoria\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 41: \n",
      " - ID: com.apple.speech.synthesis.voice.xander\n",
      " - Name: Xander\n",
      " - Languages: ['nl_NL']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 42: \n",
      " - ID: com.apple.speech.synthesis.voice.yelda\n",
      " - Name: Yelda\n",
      " - Languages: ['tr_TR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 43: \n",
      " - ID: com.apple.speech.synthesis.voice.yuna\n",
      " - Name: Yuna\n",
      " - Languages: ['ko_KR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 44: \n",
      " - ID: com.apple.speech.synthesis.voice.yuri\n",
      " - Name: Yuri\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 45: \n",
      " - ID: com.apple.speech.synthesis.voice.zosia\n",
      " - Name: Zosia\n",
      " - Languages: ['pl_PL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 46: \n",
      " - ID: com.apple.speech.synthesis.voice.zuzana\n",
      " - Name: Zuzana\n",
      " - Languages: ['cs_CZ']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the voices\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Loop over voices and print their descriptions\n",
    "for index, voice in enumerate(voices):\n",
    "    print(\"Voice {}: \".format(index))\n",
    "    print(\" - ID: %s\" % voice.id)\n",
    "    print(\" - Name: %s\" % voice.name)\n",
    "    print(\" - Languages: %s\" % voice.languages)\n",
    "    print(\" - Gender: %s\" % voice.gender)\n",
    "    print(\" - Age: %s\" % voice.age)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5UgYSzWX4LSS"
   },
   "outputs": [],
   "source": [
    "### Voice properties    \n",
    "\n",
    "# Speed percent (can go over 100)\n",
    "engine.setProperty(name = 'rate', value = 180)    \n",
    "\n",
    "# Volume 0-1\n",
    "engine.setProperty(name = 'volume', value = 0.9)\n",
    "\n",
    "# Voice ID\n",
    "en_voice_id = \"com.apple.speech.synthesis.voice.daniel.premium\"\n",
    "engine.setProperty('voice', en_voice_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PeiVQ4Fk4LSW"
   },
   "outputs": [],
   "source": [
    "# Convert the text to speech\n",
    "engine.say(\"You've got mail!\")\n",
    "engine.say(\"The pyttsx3 module supports native Windows and Mac speech APIs but also supports espeak, making it the best available text-to-speech package.\")\n",
    "engine.runAndWait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0Fu37lE4LSc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 5. Finalize your Conversational Based Agent\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to put everything together so you can do speech-to-text, text-to-text, and text-to-speech at the same time. For this, you will create a button which after pushing you can speak and your model will speck to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w4jgCKIc4LSd"
   },
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from text_to_text import text_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conversational based agent \n",
    "def agent():\n",
    "    button = widgets.Button(description=\"Click Here for Talking!\")\n",
    "    output = widgets.Output()\n",
    "    display(button, output)\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            # Speech recognition\n",
    "            print(\"Say Something...\")\n",
    "            text = speech_recognizer.recognize_once().text\n",
    "            print(\" - YOU SAID: \", text)\n",
    "            # Text-to-text\n",
    "            response = text_to_text(text, enc_model, dec_model, str_to_tokens, preprocess_text, tokenizer, maxlen_answers)\n",
    "            print(\" + AGENT: \", response)\n",
    "            # Text to speech\n",
    "            engine.say(response)\n",
    "            engine.runAndWait() \n",
    "            print(\"\")\n",
    "    button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c676eb67fc34ecfbd961b6358605368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click Here for Talking!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebc987aef2b493badcf7b473460c79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk to your agent\n",
    "agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrdHkW7P4LSt"
   },
   "source": [
    "# Good Job!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Conversational Based Agent.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
